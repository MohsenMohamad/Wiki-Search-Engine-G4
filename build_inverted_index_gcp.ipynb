{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00e032c",
   "metadata": {
    "id": "a00e032c"
   },
   "source": [
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac36d3a",
   "metadata": {
    "id": "5ac36d3a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-1ddb  GCE       4                                       RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "51cf86c5"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf199e6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf199e6a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "6e3951e1-e666-480c-90e3-13024bdf37d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f56ecd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "d8f56ecd",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f6ad17fc-4f59-4c44-cf2c-3234d781b4ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a897f2",
   "metadata": {
    "id": "38a897f2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-jar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Jan 13 17:08 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47900073",
   "metadata": {
    "id": "47900073",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_stemming = False\n",
    "stemmer = None\n",
    "if apply_stemming:\n",
    "    stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980e62a5",
   "metadata": {
    "id": "980e62a5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If apply_stemming is True, then another bucket is selcted\n",
    "bucket_name = 'final-project-full-corpus-tf-sorted-64500'\n",
    "if apply_stemming:\n",
    "    bucket_name = 'final-project-full-corpus-tf-sorted-64500-stem'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh' and not (b.name.startswith(\"postings_gcp\") or b.name.startswith(\"pr\")):\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c3f5e",
   "metadata": {
    "id": "582c3f5e"
   },
   "source": [
    "# Building an inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f2044",
   "metadata": {
    "id": "481f2044"
   },
   "source": [
    "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c523e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4c523e7",
    "outputId": "8680fd78-e5de-4f25-f1a7-d91081238932",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66697dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_doc_data = parquetFile.select(\"id\", \"title\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_doc_data = parquetFile.select(\"id\", \"text\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b305b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_doc_data_raw = parquetFile.select(\"id\", \"anchor_text\").rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e2971",
   "metadata": {
    "id": "0d7e2971"
   },
   "source": [
    "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82881fbf",
   "metadata": {
    "id": "82881fbf",
    "outputId": "a6c44df2-8b55-429b-d9d6-a4f69fb015bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6348910"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of wiki pages\n",
    "parquetFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701811af",
   "metadata": {
    "id": "701811af"
   },
   "source": [
    "Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c101a8",
   "metadata": {
    "id": "57c101a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c259c402",
   "metadata": {
    "id": "c259c402"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540c727",
   "metadata": {
    "id": "5540c727"
   },
   "source": [
    "Auxiliary functions to calculate different parameters of the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ad8fea",
   "metadata": {
    "id": "f3ad8fea",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-token2bucket",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "###########################################################\n",
    "def word_count(text, id, use_stemming=False, stemmer=None):\n",
    "  ''' Count the frequency of each word in `text` (tf) that is not included in\n",
    "  `all_stopwords` and return entries that will go into our posting lists.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    text: str\n",
    "      Text of one document\n",
    "    id: int\n",
    "      Document id\n",
    "    apply_stemming: boolean\n",
    "      Flag that notes whether to apply stemming\n",
    "    stemmer: PorterStemmer or None\n",
    "      PorterStemmer object if apply_stemming=True else None object\n",
    "  Returns:\n",
    "  --------\n",
    "    list of tuples\n",
    "      A list of (token, (doc_id, tf)) pairs\n",
    "  '''\n",
    "  word_counts = {}\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "  if use_stemming:\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "  for token in tokens:\n",
    "      word_counts[token] = word_counts.get(token, 0) + 1\n",
    "  return [(token, (id, word_counts[token])) for token in word_counts]\n",
    "\n",
    "####################################\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "  ''' Returns a sorted posting list by tf in descending order.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    unsorted_pl: list of tuples\n",
    "      A list of (wiki_id, tf) tuples\n",
    "  Returns:\n",
    "  --------\n",
    "    list of tuples\n",
    "      A sorted posting list.\n",
    "  '''\n",
    "  return sorted(unsorted_pl, key=itemgetter(1), reverse=True)\n",
    "\n",
    "###########################\n",
    "def calculate_df(postings):\n",
    "  ''' Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  return postings.mapValues(len)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "#####################################################\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "  ''' A function that partitions the posting lists into buckets, writes out\n",
    "  all posting lists in a bucket to disk, and returns the posting locations for\n",
    "  each bucket. Partitioning should be done through the use of `token2bucket`\n",
    "  above. Writing to disk should use the function  `write_a_posting_list`, a\n",
    "  static method implemented in inverted_index_colab.py under the InvertedIndex\n",
    "  class.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each item is a (w, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and\n",
    "      offsets its posting list was written to. See `write_a_posting_list` for\n",
    "      more details.\n",
    "  '''\n",
    "  bucket_id_postings = postings.map(lambda posting: (token2bucket_id(posting[0]), [(posting[0], posting[1])]))\n",
    "  reduced_postings = bucket_id_postings.reduceByKey(lambda x, y: x + y)\n",
    "  return reduced_postings.map(lambda posting: InvertedIndex.write_a_posting_list(posting, base_dir, bucket_name))\n",
    "\n",
    "#####################################################\n",
    "def tokenize(text, use_stemming=False, stemmer=None):\n",
    "  \"\"\"\n",
    "    This function tokenizes a text into a list of tokens. In addition, it filters stopwords and performs stemming.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string, represting the text to tokenize.\n",
    "    apply_stemming: boolean flag that notes whether to apply stemming\n",
    "    stemmer: PorterStemmer object if apply_stemming=True else None object\n",
    "    Returns:\n",
    "    -----------\n",
    "    list of tokens.\n",
    "  \"\"\"\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "  if use_stemming:\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "  return tokens\n",
    "\n",
    "#########################\n",
    "def calculate_idf(index):\n",
    "  # Calculate idf values for each term in index\n",
    "  idf_token_dict = defaultdict(float)       # returns zero if key is not found in dictionary\n",
    "  num_docs_corpus = len(index.dl)\n",
    "  for token in index.df.keys():\n",
    "    idf_token_dict[token] = math.log(num_docs_corpus / index.df[token], 10)\n",
    "  return idf_token_dict\n",
    "\n",
    "##############################\n",
    "def group_links(posting_list):\n",
    "  # Auxiliary function to group anchor's links\n",
    "  link_counter = {}\n",
    "  for pair in posting_list:\n",
    "    link_counter[pair[0]] = link_counter.get(pair[0], 0) + 1\n",
    "  return sorted([(key, link_counter[key]) for key in link_counter.keys()], key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4703f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary of doc_id and its title\n",
    "id_title_pairs = title_doc_data.map(lambda row: (row[0], row[1]))\n",
    "id_title_dict = id_title_pairs.collectAsMap()\n",
    "\n",
    "with open('titles.pkl', 'wb') as f:\n",
    "    pickle.dump(id_title_dict, f)\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob('postings_gcp/titles.pkl')\n",
    "blob.upload_from_filename('titles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c8764e",
   "metadata": {
    "id": "55c8764e",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-index_construction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "f18c6223-63db-4472-cf8d-fef580dfed63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Builds title inverted index\n",
    "word_counts = title_doc_data.flatMap(lambda x: word_count(x[1], x[0], apply_stemming, stemmer))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) > 0)    # filter out blank titles after tokenization\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "_ = partition_postings_and_write(postings_filtered, 'postings_gcp/title').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab3296f4",
   "metadata": {
    "id": "ab3296f4",
    "nbgrader": {
     "grade": true,
     "grade_id": "collect-posting",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/title/'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2cfb6",
   "metadata": {
    "id": "a5d2cfb6",
    "outputId": "4b83104c-779e-42e2-a1eb-93b3e84e945e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted_title = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_title.posting_locs = super_posting_locs\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_title.df = w2df_dict\n",
    "# write the global stats out\n",
    "inverted_title.write_index('postings_gcp', 'title_index', bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfb1f4",
   "metadata": {
    "id": "94bfb1f4"
   },
   "outputs": [],
   "source": [
    "# Builds body inverted index\n",
    "word_counts = body_doc_data.flatMap(lambda x: word_count(x[1], x[0], apply_stemming, stemmer))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) > 50)           # filter out rare words\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# Calculates length of all documents puts them in dictionary of doc_id and its length, in addition\n",
    "# calculates avg. length of all documents (to be used by BM25 ranking function)\n",
    "index_body_tok = body_doc_data.map(lambda row: (row[0], tokenize(row[1], apply_stemming, stemmer)))\n",
    "index_body_tok = index_body_tok.filter(lambda row: len(row[1]) > 0)\n",
    "w2dl = index_body_tok.map(lambda x: (x[0], len(x[1])))\n",
    "dl_dict = w2dl.collectAsMap()\n",
    "\n",
    "sum_and_count = w2dl.map(lambda row: (row[1], 1)).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "sum_dl_body = sum_and_count[0]\n",
    "N_docs_body = sum_and_count[1]\n",
    "AVG_DL_body = sum_dl_body / N_docs_body\n",
    "\n",
    "_ = partition_postings_and_write(postings_filtered, 'postings_gcp/body').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdfc8e",
   "metadata": {
    "id": "00cdfc8e"
   },
   "outputs": [],
   "source": [
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/body/'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d559c6f",
   "metadata": {
    "id": "6d559c6f"
   },
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted_body = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_body.posting_locs = super_posting_locs\n",
    "# Add token - df dictionary to the inverted index\n",
    "inverted_body.df = w2df_dict\n",
    "# Add doc_id - doc length dictionary to the inverted index\n",
    "inverted_body.dl = dl_dict\n",
    "# Average length of doc in inverted index\n",
    "inverted_body.avg_dl = AVG_DL_body\n",
    "# write the global stats out\n",
    "inverted_body.write_index('postings_gcp', 'body_index', bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8917b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds dictionary of term and it's IDF value for body inverted index\n",
    "term_idf_body_dict = calculate_idf(inverted_body)\n",
    "\n",
    "with open('term_idf_body.pkl', 'wb') as f:\n",
    "    pickle.dump(term_idf_body_dict, f)\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob('postings_gcp/term_idf_body.pkl')\n",
    "blob.upload_from_filename('term_idf_body.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd58032",
   "metadata": {
    "id": "8fd58032",
    "outputId": "486133ae-85c5-419c-8b18-7c6302ad2458"
   },
   "outputs": [],
   "source": [
    "# Builds anchor inverted index\n",
    "anchor_doc_data = anchor_doc_data_raw.flatMap(lambda x: x[1])\n",
    "word_counts = anchor_doc_data.flatMap(lambda x: word_count(x[1], x[0], apply_stemming, stemmer))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) > 0)    # filter out empty link lists after tokenization\n",
    "grouped_postings = postings_filtered.map(lambda x: (x[0], group_links(x[1])))\n",
    "w2df = calculate_df(grouped_postings)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "_ = partition_postings_and_write(grouped_postings, 'postings_gcp/anchor').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da4e29",
   "metadata": {
    "id": "07da4e29"
   },
   "outputs": [],
   "source": [
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/anchor/'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b431e17",
   "metadata": {
    "id": "0b431e17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted_anchor = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_anchor.posting_locs = super_posting_locs\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_anchor.df = w2df_dict\n",
    "# write the global stats out\n",
    "inverted_anchor.write_index('postings_gcp', 'anchor_index', bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "  temp_edges = pages.map(lambda x: [(x[0], y[0]) for y in x[1]])\n",
    "  edges = temp_edges.flatMap(lambda x: x).distinct()\n",
    "  temp_vertices = edges.map(lambda x: [(x[0], x[0]), (x[1], x[1])])\n",
    "  vertices = temp_vertices.flatMap(lambda x: x).distinct()\n",
    "  return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7468c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the graph \n",
    "edges, vertices = generate_graph(anchor_doc_data_raw)\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n",
    "pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns pr into dictionary of doc_id and its PageRank\n",
    "pgrank_rdd = pr.rdd\n",
    "id_pgrank_pairs = pgrank_rdd.map(lambda row: (row[0], row[1]))\n",
    "id_pgrank_dict = id_pgrank_pairs.collectAsMap()\n",
    "\n",
    "with open('pagerank.pkl', 'wb') as f:\n",
    "    pickle.dump(id_pgrank_dict, f)\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob('postings_gcp/pagerank.pkl')\n",
    "blob.upload_from_filename('pagerank.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfff9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using user page views (as opposed to spiders and automated traffic) for the month of August 2021\n",
    "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "p = Path(pv_path) \n",
    "pv_name = p.name\n",
    "pv_temp = f'{p.stem}-4dedup.txt'\n",
    "pv_clean = f'{p.stem}.pkl'\n",
    "# Download the file (2.3GB) \n",
    "!wget -N $pv_path\n",
    "# Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
    "# total number of page views (5). Then, remove lines with article id or page \n",
    "# view values that are not a sequence of digits.\n",
    "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
    "# Create a Counter (dictionary) that sums up the pages views for the same \n",
    "# article, resulting in a mapping from article id to total page views.\n",
    "wid2pv = Counter()\n",
    "with open(pv_temp, 'rt') as f:\n",
    "  for line in f:\n",
    "    parts = line.split(' ')\n",
    "    wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "# Convert counter to defaultdict\n",
    "page_view_dict = defaultdict(int)\n",
    "for doc_id, view in wid2pv.items():\n",
    "  page_view_dict[doc_id] = view\n",
    "\n",
    "with open(\"pageview.pkl\", 'wb') as f:\n",
    "  pickle.dump(page_view_dict, f)\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob('postings_gcp/pageview.pkl')\n",
    "blob.upload_from_filename('pageview.pkl')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
